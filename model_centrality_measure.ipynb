{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd\n",
    "\n",
    "\n",
    "nc = np.load('./output/THREE_LOSSES_229_NC_ALL/fold4_cbt.npy')\n",
    "adhd = np.load('./output/ADHD_THREE_LOSS_TRAIN/fold4_cbt.npy')\n",
    "lmci = np.load('./output/LMCI_THREE_LOSS_TRAIN/fold4_cbt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asd\n",
    "asd = np.load('./output/THREE_LOSSES_229/fold0_cbt.npy')\n",
    "asd_test = np.load('./output/test_data/fold0_all_cbts_asd.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc\n",
    "asd = np.load('./output/THREE_LOSSES_229_NC_ALL/fold0_cbt.npy')\n",
    "asd_test = np.load('./output/test_data/fold0_all_cbts_nc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adhd\n",
    "asd = np.load('./output/ADHD_THREE_LOSS_TRAIN/fold0_cbt.npy')\n",
    "asd_test = np.load('./output/ADHD_THREE_LOSS_TEST/fold0_all_cbts.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lmci\n",
    "asd = np.load('./output/LMCI_THREE_LOSS_TRAIN/fold3_cbt.npy')\n",
    "asd_test = np.load('./output/LMCI_THREE_LOSS_TEST/fold0_all_cbts.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07724070826578083"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import floyd_warshall\n",
    "\n",
    "# Compute the shortest paths between all pairs of nodes\n",
    "# Using floyd_warshall from scipy, which automatically considers 'asd' as a weighted graph\n",
    "# We assume 'asd' is already defined and represents the adjacency matrix of the network\n",
    "dist_matrix = floyd_warshall(asd, directed=False)\n",
    "\n",
    "# Replace infinity values with zeros to avoid division by zero in later calculations\n",
    "# Infinity values occur for pairs of nodes between which no path exists\n",
    "dist_matrix[dist_matrix == np.inf] = 0\n",
    "\n",
    "# Calculate the information centrality for each node\n",
    "n_nodes = asd.shape[0]\n",
    "info_centrality = np.zeros(n_nodes)\n",
    "\n",
    "for i in range(n_nodes):\n",
    "    # Exclude self-loops\n",
    "    valid_distances = dist_matrix[i][dist_matrix[i] > 0]\n",
    "    \n",
    "    # Harmonic mean of distances for node i\n",
    "    if valid_distances.size > 0:\n",
    "        info_centrality[i] = 1 / np.mean(1 / valid_distances)\n",
    "\n",
    "info_centrality.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02857142947614193"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_degree_sum = np.sum(asd)\n",
    "n=35\n",
    "\n",
    "# Corrected Laplacian centrality calculation, focusing on non-negative contributions\n",
    "C_Laplacian_corrected = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    # Directly using degree might already ensure non-negativity but let's ensure the calculation is clearly non-negative\n",
    "    C_Laplacian_corrected[i] = np.sum(asd[i]) / total_degree_sum\n",
    "\n",
    "C_Laplacian_corrected.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node strength 3.3048754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9608889316208661, 0.09913314378828565)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = (asd + asd.T) / 2\n",
    "np.fill_diagonal(asd, 0)\n",
    "\n",
    "for i in range(asd_test.shape[2]):\n",
    "    asd_test[:, :, i] = (asd_test[:, :, i] + asd_test[:, :, i].T) / 2\n",
    "    np.fill_diagonal(asd_test[:, :, i], 0)\n",
    "\n",
    "# Calculate node strength for the main matrix\n",
    "asd_strength = np.sum(asd, axis=1)\n",
    "\n",
    "print(\"node strength\", np.mean(asd_strength))\n",
    "\n",
    "# Initialize an array to store the mean and std of strength differences for each test matrix\n",
    "mean_strength_diffs = np.zeros(asd_test.shape[2])\n",
    "std_strength_diffs = np.zeros(asd_test.shape[2])\n",
    "\n",
    "# Calculate node strength for each test matrix and their differences from the main matrix\n",
    "for i in range(asd_test.shape[2]):\n",
    "    # Calculate node strength for the current test matrix\n",
    "    test_matrix_strength = np.sum(asd_test[:, :, i], axis=1)\n",
    "    \n",
    "    # Calculate element-wise difference in node strength between the current test matrix and the main matrix\n",
    "    strength_diffs = np.abs(test_matrix_strength - asd_strength)\n",
    "    \n",
    "    # Calculate the mean and std of the strength differences\n",
    "    mean_strength_diffs[i] = np.mean(strength_diffs)\n",
    "    std_strength_diffs[i] = np.std(strength_diffs)\n",
    "\n",
    "np.mean(mean_strength_diffs), np.std(std_strength_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1605559050085541"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.10420735836853082, 0.03982993397295201, 0.24534111144493254, 0.28904792516050254, 0.12435319609585263])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.10420735836853082, 0.23696557820263336)\n",
    "(0.03982993397295201, 0.09707140581147687)\n",
    "(0.24534111144493254, 1.0640493673628049)\n",
    "(0.28904792516050254, 0.4822504313824283)\n",
    "(0.12435319609585263, 0.1741636406797108)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16804128110092748\n"
     ]
    }
   ],
   "source": [
    "def calculate_eigenvector_centrality(matrix, max_iter=100, tol=1.0e-6):\n",
    "    n = matrix.shape[0]\n",
    "    # Initialize the eigenvector with equal values (e.g., 1/n)\n",
    "    x = np.ones(n) / n\n",
    "    \n",
    "    # Normalize the matrix by its largest absolute eigenvalue to ensure convergence\n",
    "    eigenvalues, _ = np.linalg.eig(matrix)\n",
    "    lambda_max = np.max(np.abs(eigenvalues))\n",
    "    A_norm = matrix / lambda_max\n",
    "    \n",
    "    # Power iteration: repeatedly update the eigenvector estimate\n",
    "    for _ in range(max_iter):\n",
    "        x_next = A_norm.dot(x)\n",
    "        x_next /= np.linalg.norm(x_next)  # Normalize the vector\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        x = x_next\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Example usage with a dummy adjacency matrix\n",
    "# Replace `asd` with your actual matrix\n",
    "asd = np.random.rand(35, 35)\n",
    "\n",
    "# Calculate eigenvector centrality\n",
    "centrality = calculate_eigenvector_centrality(asd)\n",
    "print(np.mean(centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05714285714285714\n"
     ]
    }
   ],
   "source": [
    "def adjacency_matrix_to_list(matrix):\n",
    "    adjacency_list = {}\n",
    "    for i, row in enumerate(matrix):\n",
    "        adjacency_list[i] = [j for j, connected in enumerate(row) if connected]\n",
    "    return adjacency_list\n",
    "\n",
    "def brandes_algorithm(graph):\n",
    "    # Initialize betweenness centrality dictionary\n",
    "    C_b = {v: 0 for v in graph}\n",
    "    for s in graph:\n",
    "        # Initialization\n",
    "        S = []\n",
    "        P = {v: [] for v in graph}  # Predecessors\n",
    "        g = {v: 0 for v in graph}   # Number of shortest paths\n",
    "        g[s] = 1\n",
    "        d = {v: -1 for v in graph}  # Distance from s\n",
    "        d[s] = 0\n",
    "        Q = []  # Queue for BFS\n",
    "        Q.append(s)\n",
    "        \n",
    "        # BFS from s\n",
    "        while Q:\n",
    "            v = Q.pop(0)\n",
    "            S.append(v)\n",
    "            for w in graph[v]:\n",
    "                # Path discovery\n",
    "                if d[w] < 0:\n",
    "                    Q.append(w)\n",
    "                    d[w] = d[v] + 1\n",
    "                # Path counting\n",
    "                if d[w] == d[v] + 1:\n",
    "                    g[w] += g[v]\n",
    "                    P[w].append(v)\n",
    "        \n",
    "        # Accumulation\n",
    "        delta = {v: 0 for v in graph}\n",
    "        while S:\n",
    "            w = S.pop()\n",
    "            for v in P[w]:\n",
    "                delta[v] += (g[v] / g[w]) * (1 + delta[w])\n",
    "            if w != s:\n",
    "                C_b[w] += delta[w]\n",
    "                \n",
    "    return C_b\n",
    "\n",
    "graph = adjacency_matrix_to_list(asd)\n",
    "\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = brandes_algorithm(graph)\n",
    "print(np.mean(list(betweenness_centrality.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_centrality 5.092946269416858e-05\n",
      "Mean Difference in Betweenness Centrality: 1.0913456291607553e-05\n",
      "Standard Deviation in Betweenness Centrality: 2.089767778579806e-05\n"
     ]
    }
   ],
   "source": [
    "def calculate_betweenness_centrality(matrix):\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    return centrality\n",
    "\n",
    "# Calculate centrality for the trained matrix\n",
    "trained_centrality = calculate_betweenness_centrality(asd)\n",
    "print(\"trained_centrality\", np.mean(list(trained_centrality.values())))\n",
    "\n",
    "# Calculate centralities for each testing set matrix and compute differences\n",
    "differences = []\n",
    "for i in range(asd_test.shape[2]):\n",
    "    test_centrality = calculate_betweenness_centrality(asd_test[:, :, i])\n",
    "    # Compute difference in centrality values\n",
    "    # For betweenness, we're also comparing mean values as an example\n",
    "    diff = np.abs(np.mean(list(test_centrality.values())) - np.mean(list(trained_centrality.values())))\n",
    "    differences.append(diff)\n",
    "\n",
    "# Calculate overall mean and standard deviation of differences\n",
    "mean_difference = np.mean(differences)\n",
    "std_difference = np.std(differences)\n",
    "\n",
    "print(\"Mean Difference in Betweenness Centrality:\", mean_difference)\n",
    "print(\"Standard Deviation in Betweenness Centrality:\", std_difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
